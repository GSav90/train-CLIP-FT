{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f91838-b496-4a2e-b411-be32afa1b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from PIL import Image\n",
    "from urllib import request\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision, torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import time\n",
    "# proxy=\"http://sysproxy.wal-mart.com:8080\"\n",
    "# os.environ['http_proxy'] = proxy \n",
    "# os.environ['HTTP_PROXY'] = proxy\n",
    "# os.environ['https_proxy'] = proxy\n",
    "# os.environ['HTTPS_PROXY'] = proxy\n",
    "\n",
    "\n",
    "# export HTTP_PROXY=http://sysproxy.wal-mart.com:8080\n",
    "# export HTTPS_PROXY=http://sysproxy.wal-mart.com:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e17c3ba-350c-453b-a447-73e4c1c9e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: DtypeWarning: Columns (0,27,49,73,144,166,182,193,231,243,270,276,327,328,330,345,390,393,400,402,404,406,423,426,427,438,445,446,452,459,460,466,469,483,488,489,490,491,493,494,498,500,501,502,503,505,507,509,510,511,513,519,528,534,537,546,575,580,586,591,594,605,614,619,621,627,629,630,639,640,641,642,660,661,663,666,667,672,676,684,685,687,689,694,697,699,700,701,705,706,707,710,712,713,715,717,719,724,726,730,732,733,736,737,740,741,743,744,745,746,748,750,752,753,755,756,760,761,762,763,765,767,770,771,772,778,780,781,782,783,785,788,793,794,796,797,798,799,800,804,805,806,807,808,809,811,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,832,833,834,835,836,837,838,839,840,841,842,844,845,846,848,849,850,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,868,869,870,871,872,874,877,881,882,885,889,891,892,907,908,911,916,917,923,924,931,933,934,949,951,957,958,965,967,968,970,973,974,978,984,986,987,994,1003,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1020,1024,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1047,1050,1051,1055,1057,1058,1059,1061,1064,1065,1067,1071,1074,1079,1080,1085,1092,1093,1109,1110,1113,1114,1123,1135,1136,1140,1143,1145,1147,1153,1154,1155,1156,1157,1159,1169,1170,1172,1175,1179,1180,1182,1184,1187,1190,1191,1223,1224,1227,1230,1233,1235,1236,1239,1240,1242,1247,1248,1260,1261,1269,1271,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1291,1292,1293,1294,1297,1298,1300,1301,1302,1303,1304,1305,1307,1308,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1324,1325,1326,1327,1328,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1354,1355,1356,1357,1358,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1406,1407,1408,1409,1410,1411,1412,1413,1414) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gtin's with no description: ['00016000124790' '00044700360019' '00703820010944']\n",
      "Shape before removing gtin's10661\n",
      "Shape after removing gtin's10132\n",
      "Shape after removing na from product name column 10132\n",
      "Gtin's with no description: ['00016000124790' '00044700360019' '00703820010944']\n",
      "Shape before removing gtin's34401\n",
      "Shape after removing gtin's32691\n",
      "Shape after removing na from product name column 32691\n",
      "Gtin's with no description: ['00016000124790' '00044700360019' '00703820010944']\n",
      "Shape before removing gtin's7914\n",
      "Shape after removing gtin's7520\n",
      "Shape after removing na from product name column 7520\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# import pandas as pd\n",
    "# gtin_mapping=pd.read_csv(os.path.join(os.getcwd(),\"dvc-manual/520_gtin_product_name.csv\"))\n",
    "# [gtin_mapping[gtin_mapping[\"gtin\"]==gtin_mapping[\"gtin\"].value_counts().index[1]][\"product_name\"].iloc[i] for i in range(0,4)]\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    CLEANR = re.compile('<.*?>') # remove html tags\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    pattern= r\"\\d*\\.\\d+\"# r'[0-9]' # remove decimal numbers\n",
    "    cleantext=re.sub(pattern, \"\",cleantext)\n",
    "    pattern= r'[0-9]' # remove any digits\n",
    "    cleantext=re.sub(pattern, \"\",cleantext)\n",
    "    return cleantext\n",
    "\n",
    "def get_mapping():\n",
    "    \n",
    "    gtin_mapping=pd.read_csv(os.path.join(os.getcwd(),\"dvc-manual/gtin_attr.csv\"))\n",
    "    desc_columns = [col for col in gtin_mapping.columns if 'desc' in col.lower() or \"name\" in col.lower() or \"date\" in col.lower()]\n",
    "    desc_columns= [\"gtin\",\"KARF Picker Description\",\"Product Long Description\",\"Short Description\",\"Product Name\" ]\n",
    "    gtin_mapping= gtin_mapping.loc[:,desc_columns]\n",
    "    \n",
    "    \n",
    "    gtin_mapping[\"Product Long Description\"]=gtin_mapping.apply(lambda x: cleanhtml(str(x[\"Product Long Description\"])), axis=1)\n",
    "    gtin_mapping[\"Short Description\"]=gtin_mapping.apply(lambda x: cleanhtml(str(x[\"Short Description\"])), axis=1)\n",
    "    gtin_mapping[\"Product Name\"]=gtin_mapping.apply(lambda x: cleanhtml(str(x[\"Product Name\"])), axis=1)\n",
    "    gtin_mapping=gtin_mapping.rename(columns={\"Product Long Description\": \"desc_long\", \"Short Description\": \"desc_short\", \"Product Name\": \"name\",\"KARF Picker Description\":\"desc_karf\" })\n",
    "\n",
    "    gtin_mapping[\"desc_long\"]=gtin_mapping.apply(lambda x: str(x[\"desc_long\"]).replace(\"|\", \"\"),axis=1)\n",
    "    gtin_mapping[\"desc_short\"]=gtin_mapping.apply(lambda x: str(x[\"desc_short\"]).replace(\"|\", \"\"),axis=1)\n",
    "    gtin_mapping[\"desc_karf\"]=gtin_mapping.apply(lambda x: str(x[\"desc_karf\"]).replace(\"|\", \"\"),axis=1)\n",
    "    gtin_mapping[\"name\"]=gtin_mapping.apply(lambda x: str(x[\"name\"]).replace(\"|\", \"\"),axis=1)\n",
    "\n",
    "    gtin_mapping[\"gtin\"]=gtin_mapping[\"gtin\"].apply(lambda x: str(x).zfill(14))\n",
    "    \n",
    "    return gtin_mapping\n",
    "\n",
    "def create_dataframe(path,label_col):\n",
    "    loader= torchvision.datasets.ImageFolder(root=path)\n",
    "    \n",
    "    df= pd.DataFrame(loader.imgs, columns= [\"img_path\", \"label\"])\n",
    "    df[\"gtin\"]= df.apply(lambda x: x[\"img_path\"].split(\"/\")[-2],axis=1)\n",
    "    df[\"gtin\"]=df[\"gtin\"].apply(lambda x: str(x).zfill(14))\n",
    "    \n",
    "    gtin_mapping= get_mapping()\n",
    "    \n",
    "    df= df.merge(gtin_mapping, left_on=['gtin'], right_on=[\"gtin\"], how = \"left\")\n",
    "    no_desc_gtin= df[df['name'].isna()].gtin.unique()\n",
    "    print(f\"Gtin's with no description: {no_desc_gtin}\")\n",
    "    print(f\"Shape before removing gtin's{df.shape[0]}\")\n",
    "    df= df[~df[\"gtin\"].isin(no_desc_gtin)]\n",
    "    print(f\"Shape after removing gtin's{df.shape[0]}\")\n",
    "    df = df[df[label_col].notna()]\n",
    "    print(f\"Shape after removing na from product name column {df.shape[0]}\")\n",
    "    key_list=[k for k in range(df.shape[0])]\n",
    "    df[\"key\"]= key_list\n",
    "    return df\n",
    "\n",
    "label_col= \"name\"\n",
    "test_path= os.path.join(os.getcwd(),'dvc-manual/gtin_60/data/prep/80gtin_allsubfolders/splitfolders/test')\n",
    "test_df=create_dataframe(test_path, label_col)\n",
    "\n",
    "train_path= os.path.join(os.getcwd(),'dvc-manual/gtin_60/data/prep/80gtin_allsubfolders/splitfolders/train')\n",
    "train_df=create_dataframe(train_path, label_col)\n",
    "\n",
    "val_path= os.path.join(os.getcwd(),'dvc-manual/gtin_60/data/prep/80gtin_allsubfolders/splitfolders/val')\n",
    "val_df=create_dataframe(val_path, label_col)\n",
    "\n",
    "## remove gtin's with no description available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8891468-3f6e-475a-9caa-121ea33b5eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nutella Chocolate Hazelnut Spread Perfect on Pancakes  oz Jar'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # batch_num=0\n",
    "# # for batch in train_dataloader:\n",
    "# #     batch_num+=1\n",
    "# #     if batch_num==52:\n",
    "# #         print(batch[1])\n",
    "# from transformers import CLIPTokenizer, CLIPTextModel\n",
    "# text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "# vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "from random import randint, choice\n",
    "descriptions = train_dataloader.dataset.caption[0].split('\\n')\n",
    "descriptions = list(filter(lambda t: len(t) > 0, descriptions))\n",
    "description = choice(descriptions)\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "66db2b47-dcd4-4438-a7c8-ec677b414550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>gtin</th>\n",
       "      <th>desc_karf</th>\n",
       "      <th>desc_long</th>\n",
       "      <th>desc_short</th>\n",
       "      <th>name</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path  label            gtin  \\\n",
       "0  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "1  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "2  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "3  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "4  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "\n",
       "         desc_karf                                          desc_long  \\\n",
       "0  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "1  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "2  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "3  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "4  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "\n",
       "                                          desc_short  \\\n",
       "0  The great taste of Nutella makes weekend break...   \n",
       "1  The great taste of Nutella makes weekend break...   \n",
       "2  The great taste of Nutella makes weekend break...   \n",
       "3  The great taste of Nutella makes weekend break...   \n",
       "4  The great taste of Nutella makes weekend break...   \n",
       "\n",
       "                                                name  key  \n",
       "0  Nutella Chocolate Hazelnut Spread Perfect on P...    0  \n",
       "1  Nutella Chocolate Hazelnut Spread Perfect on P...    1  \n",
       "2  Nutella Chocolate Hazelnut Spread Perfect on P...    2  \n",
       "3  Nutella Chocolate Hazelnut Spread Perfect on P...    3  \n",
       "4  Nutella Chocolate Hazelnut Spread Perfect on P...    4  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list=[k for k in range(train_df.shape[0])]\n",
    "train_df[\"key\"]= key_list\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1b10f-fece-4d64-a6e9-067012613676",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- CLIP encoder, CLIP text encoder, CLIP tokenizer, CLIP processor\n",
    "\n",
    "1. Prepare data into the required format\n",
    "2. Load text encoder and image encoder\n",
    "    Image processor CLIP preprocess\n",
    "    CLIP tokenizer\n",
    "        Default: clip.tokenize\n",
    "        Huggingface: \n",
    "        \n",
    "root_folder\n",
    "    img1.png\n",
    "    img1.txt\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10e6958b-8116-4a72-b011-5e1211b0d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "BATCH_SIZE=64\n",
    "\n",
    "# class image_caption_dataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "\n",
    "#         self.images = df[\"img_path\"].tolist()\n",
    "#         self.caption = df[\"name\"].tolist()\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.caption)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images = preprocess(Image.open(self.images[idx])) #preprocess from clip.load\n",
    "#         caption = self.caption[idx]\n",
    "#         return images,caption\n",
    "\n",
    "\n",
    "# class image_caption_dataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "\n",
    "#         self.images = df[\"img_path\"].tolist()\n",
    "#         self.caption = df[\"name\"].tolist()\n",
    "#         self.device = (\n",
    "#             \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         )  # If using GPU then use mixed precision training.\n",
    "#         _, self.preprocess = clip.load(\n",
    "#             \"ViT-B/32\", device=self.device, jit=False\n",
    "#         )  # Must set jit=False for training\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.caption)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images = self.preprocess(Image.open(self.images[idx]))  # preprocess from clip.load\n",
    "#         # caption=torch.cat([clip.tokenize(c) for c in self.caption])\n",
    "#         caption=clip.tokenize(self.caption[idx])[0]\n",
    "#         # caption = self.caption[idx]\n",
    "#         return images, caption\n",
    "\n",
    "class image_caption_dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.images = df[\"img_path\"].tolist()\n",
    "        self.caption = df[\"name\"].tolist()\n",
    "        # self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.device = (\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )  # If using GPU then use mixed precision training.\n",
    "        _, self.preprocess = clip.load(\n",
    "            \"ViT-B/32\", device=self.device, jit=False\n",
    "        )  # Must set jit=False for training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = self.preprocess(Image.open(self.images[idx]))\n",
    "        caption=clip.tokenize(self.caption[idx])[0]\n",
    "        \n",
    "        # caption = self.caption[idx]\n",
    "        # caption= self.tokenizer(self.caption[idx])[0]\n",
    "        \n",
    "        return images, caption\n",
    "\n",
    "\n",
    "dataset = image_caption_dataset(train_df.loc[:,[\"img_path\",\"name\"]])\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n",
    "dataset = image_caption_dataset(val_df.loc[:,[\"img_path\",\"name\"]])\n",
    "val_loader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n",
    "dataset = image_caption_dataset(test_df.loc[:,[\"img_path\",\"name\"]])\n",
    "test_loader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "424a2a36-2c14-4f41-8345-55ef348065df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406, 27312,  3820,  ...,     0,     0,     0],\n",
      "        [49406, 27312,  3820,  ...,     0,     0,     0],\n",
      "        [49406, 27312,  3820,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [49406, 27312,  3820,  ...,     0,     0,     0],\n",
      "        [49406, 27312,  3820,  ...,     0,     0,     0],\n",
      "        [49406, 27312,  3820,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "for i,t in train_dataloader:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42a907a3-40b2-407c-b352-882cb93daf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]),\n",
       " tensor([2, 3]),\n",
       " tensor([4, 5]),\n",
       " tensor([6, 7]),\n",
       " tensor([8, 9]),\n",
       " tensor([10, 11]),\n",
       " tensor([12, 13]),\n",
       " tensor([14, 15]),\n",
       " tensor([16, 17]),\n",
       " tensor([18, 19]),\n",
       " tensor([20, 21]),\n",
       " tensor([22, 23]),\n",
       " tensor([24, 25]),\n",
       " tensor([26, 27]),\n",
       " tensor([28, 29]),\n",
       " tensor([30, 31]),\n",
       " tensor([32, 33]),\n",
       " tensor([34, 35]),\n",
       " tensor([36, 37]),\n",
       " tensor([38, 39]),\n",
       " tensor([40, 41]),\n",
       " tensor([42, 43]),\n",
       " tensor([44, 45]),\n",
       " tensor([46, 47]),\n",
       " tensor([48, 49]),\n",
       " tensor([50, 51]),\n",
       " tensor([52, 53]),\n",
       " tensor([54, 55]),\n",
       " tensor([56, 57]),\n",
       " tensor([58, 59]),\n",
       " tensor([60, 61]),\n",
       " tensor([62, 63]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dataloader.batch_size)\n",
    "n = math.ceil(train_dataloader.batch_size// 64)\n",
    "text_mbs_ids = torch.chunk(torch.arange(64), 32)\n",
    "\n",
    "for s in text_mbs_ids:\n",
    "    d = {}\n",
    "    for key in list(text.keys()):\n",
    "        d[key] = text[key][s]\n",
    "    text_mbs.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb9b329c-eba0-43d5-bdc8-6320534e9215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['( Pack) Tiger Balm Pain Relieving Patch',\n",
       "       'Ball Park White Hot Dog Buns  count  oz',\n",
       "       'Belvita Soft Baked Banana Bread Breakfast Biscuits  oz  count',\n",
       "       'Betty Crocker Super Moist White Cake Mix  oz',\n",
       "       \"Cheetos Crunchy Flamin' Hot Cheese Flavored Snacks  oz Bag\",\n",
       "       'Cheez-It Cheese Crackers Baked Snack Crackers Office and Kids Snacks Original  Oz Box',\n",
       "       'Clorox Disinfecting Wipes Bleach Free Cleaning Wipes - Crisp Lemon  ct',\n",
       "       'Coca-Cola Original Soda Pop  Fl Oz  Pack Cans',\n",
       "       'Cream of Wheat Original Hot Cereal Kosher  OZ Box  Min',\n",
       "       'Dial Antibacterial Liquid Hand Soap Refill Spring Water  fl oz',\n",
       "       'Diet Coke Caffeine-Free Cola Soda Pop  Fl Oz  Pack Bottles',\n",
       "       'Drano Max Gel Clog Remover  oz',\n",
       "       'Fixodent Complete Original Denture Adhesive Cream  oz  Pack',\n",
       "       'Folgers Classic Roast Ground Coffee -Ounce',\n",
       "       \"Gardetto's Double Roasted Garlic Rye Chips  oz Bag\",\n",
       "       'Glad Cling N Seal Plastic Food Wrap  Square Foot Roll',\n",
       "       \"Gold Bond Hydrating Lotion Diabetics' Dry Skin Relief  Oz.\",\n",
       "       'Goldfish Cheddar Crackers Snack Crackers  oz bag',\n",
       "       'Great Value % Apple Juice  Fl. Oz.',\n",
       "       'Great Value % Reduced Fat Milk Gallon  fl oz',\n",
       "       'Great Value All-Purpose Flour LB Bag',\n",
       "       'Great Value Distilled Water  Gallon',\n",
       "       'Great Value Evaporated Milk  Fl Oz',\n",
       "       'Great Value Hamburger Buns  Count  oz',\n",
       "       'Great Value Hot Dog Buns White  oz  Count',\n",
       "       'Great Value Large White Eggs  Count',\n",
       "       'Great Value Light Brown Sugar  Lb',\n",
       "       'Great Value Pure Granulated Sugar  lb',\n",
       "       'Great Value Purified Drinking Water  Fl Oz  Count Bottles',\n",
       "       'Great Value Salted Sweet Cream Butter  oz  Sticks',\n",
       "       'Great Value Steamable Cut Green Beans Frozen  oz',\n",
       "       'Great Value Steamable Whole Kernel Corn Frozen  oz',\n",
       "       'Great Value Sweetened Condensed Milk  oz',\n",
       "       'Great Value Unsalted Sweet Cream Butter  oz  Sticks',\n",
       "       'Great Value White Bread Loaf  oz  Count',\n",
       "       'Great Value Whole Vitamin D Milk Gallon  fl oz',\n",
       "       'Great Value Whole Vitamin D Milk Half Gallon  fl oz',\n",
       "       'Heinz Tomato Ketchup  oz Bottle',\n",
       "       \"Hellmann's Real Mayonnaise Condiment Real Mayo Gluten Free Made With % Cage-Free Eggs  oz\",\n",
       "       'Honey Nut Cheerios Gluten-Free Breakfast Cereal  oz',\n",
       "       'Imperial Vegetable Oil Spread  oz  Sticks',\n",
       "       'Jergens Ultra Healing Dry Skin Moisturizing Body Lotion  fl oz',\n",
       "       \"Kellogg's Club Crackers Lunch Snack Packs Original  Ct  Oz Box\",\n",
       "       'Land O Lakes® Salted Butter  lb in  Sticks',\n",
       "       'Lunchables Pizza with Pepperoni Snack Kit  oz Tray',\n",
       "       'Lysol Disinfectant Spray Sanitizing and Antibacterial Spray For Disinfecting and Deodorizing Crisp Linen  fl oz',\n",
       "       'Lysol Disinfectant Wipes Multi-Surface Antibacterial Cleaning Wipes For Disinfecting and Cleaning Lemon and Lime Blossom  Count',\n",
       "       'McCafe Premium Roast Ground Coffee Medium Roast  oz Canister',\n",
       "       'Mountain Dew Citrus Soda Pop  oz  Pack Cans',\n",
       "       'Mountain Dew Citrus Soda Pop  oz. Bottle',\n",
       "       \"Nature's Own® Honey Wheat Bread  oz. Loaf\",\n",
       "       'Newtons Soft & Fruit Chewy Fig Cookies  oz Pack',\n",
       "       'Nutella Chocolate Hazelnut Spread Perfect on Pancakes  oz Jar',\n",
       "       'OREO Chocolate Sandwich Cookies Family Size  oz',\n",
       "       'OREO Double Stuf Chocolate Sandwich Cookies Family Size  oz',\n",
       "       'On The Border Cafe Style Tortilla Chips  Oz.',\n",
       "       'Pepsi Cola Soda Pop  oz  Pack Bottles',\n",
       "       'Pepsi Cola Soda Pop  oz  Pack Cans',\n",
       "       'Philadelphia Neufchatel Cheese with / Less Fat than Cream Cheese  ct Pack  oz Bricks',\n",
       "       'Philadelphia Original Cream Cheese  oz Brick',\n",
       "       'Pringles Potato Crisps Chips Lunch Snacks On The Go Snacks Sour Cream and Onion  Oz Can',\n",
       "       'Pringles Potato Crisps Chips Lunch Snacks Snacks On The Go Original  Oz Can',\n",
       "       'Reynolds Cut-Rite Wax Paper',\n",
       "       \"Smucker's Natural Chunky Peanut Butter  Ounces\",\n",
       "       'Total Breakfast Cereal with Whole Grain Flakes  oz',\n",
       "       'Tropicana Pure Premium Original No Pulp % Orange Juice  oz Bottle',\n",
       "       'Wheat Thins Original Whole Grain Wheat Crackers Family Size  Oz',\n",
       "       'Wonder Bread Classic White Sandwich Bread Sliced White Bread  oz Loaf',\n",
       "       'York Peppermint Pattie  oz'], dtype='<U127')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes=np.unique(np.array(train_dataloader.dataset.caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fb3361c-18fa-408e-8d30-25379fad67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "\n",
    "# for train_batch in train_dataloader:\n",
    "#     image, text = train_batch\n",
    "#     print(f\"Length of image batch is {image.shape}\")\n",
    "#     print(f\"Length of text batch is {text.shape}\")\n",
    "#     n = math.ceil(len(image) // 128)\n",
    "#     print(n)\n",
    "#     image_mbs = torch.chunk(image, n)\n",
    "#     text_mbs = torch.chunk(text, n)\n",
    "\n",
    "#     # calculate original statistics\n",
    "#     # with torch.no_grad():\n",
    "#     #     ims = [F.normalize(self.model.encode_image(im), dim=1) for im in image_mbs]\n",
    "#     #     txt = [F.normalize(self.model.encode_text(t), dim=1) for t in text_mbs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a638887-2267-458d-84be-3919dc2bd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"ViT-B/32\",jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "# # from torchvision.models import resnet50\n",
    "# # img_encoder = resnet50(pretrained=True)\n",
    "# # img_encoder.fc = torch.nn.Linear(2048, 768)\n",
    "# # print(img_encoder.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8e2ce89-8d51-4bf6-8c5e-99405798c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 510/511 [10:28<00:01,  1.23s/batch, loss=4.16]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (51) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21645/2838219546.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (51) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "else :\n",
    "    clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "EPOCH=150\n",
    "\n",
    "\n",
    "print(\"Start Training\")\n",
    "for epoch in range(EPOCH):\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for list_image, list_txt in tepoch:\n",
    "        # for batch in train_dataloader:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            optimizer.zero_grad()\n",
    "            # batch_num+=1\n",
    "            # print(f'Batch number = {batch_num}')\n",
    "\n",
    "            # list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "\n",
    "            images=list_image.to(device)\n",
    "            # images= torch.stack([preprocess(Image.fromarray(img)) for img in list_image],dim=0).to(device) # omit the Image.fromarray if the images already in PIL format, change this line to images=list_image if using preprocess inside the dataset class\n",
    "            texts = clip.tokenize(list_txt).to(device)\n",
    "\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            ground_truth = torch.arange(len(list_image),dtype=torch.long,device=device)\n",
    "\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            acc_i = (torch.argmax(image_logits, 1) == ground_truth).sum()\n",
    "            acc_t = (torch.argmax(image_logits, 0) == ground_truth).sum()\n",
    "            total_loss.backward()\n",
    "            if device == \"cpu\":\n",
    "                optimizer.step()\n",
    "            else : \n",
    "                convert_models_to_fp32(model)\n",
    "                optimizer.step()\n",
    "                clip.model.convert_weights(model)\n",
    "            tepoch.set_postfix(loss=total_loss.item())\n",
    "            time.sleep(0.1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "01d7c529-b0ed-4759-ba7e-0cc0241e6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acac20b1-20e5-4ef4-8465-c67c4fda4ffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_392/1507401090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a6942-6808-4ba2-a9d8-b9d156a2e69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fa510-fb76-4c7b-aeb4-62a5c1a36ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPWrapper(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 config: dict,\n",
    "                 minibatch_size: int\n",
    "                 ):\n",
    "        \"\"\"A lightning wrapper for a CLIP model as specified in the paper.\n",
    "        Args:\n",
    "            model_name (str): A case sensitive visual model name.\n",
    "            config (dict): A dictionary containing the CLIP instantiation parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = CLIP(**config)\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.isViT = 'ViT' in self.model_name\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "    \n",
    "    # Sourced from https://github.com/PyTorchLightning/pytorch-lightning/issues/5449\n",
    "    @property\n",
    "    def num_training_steps(self) -> int:\n",
    "        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n",
    "        dataset = self.train_dataloader()\n",
    "        if self.trainer.max_steps:\n",
    "            return self.trainer.max_steps\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "\n",
    "        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)\n",
    "        if self.trainer.tpu_cores:\n",
    "            num_devices = max(num_devices, self.trainer.tpu_cores)\n",
    "\n",
    "        effective_batch_size = dataset.batch_size * self.trainer.accumulate_grad_batches * num_devices\n",
    "        return (dataset_size // effective_batch_size) * self.trainer.max_epochs\n",
    "\n",
    "    # Training loss: https://github.com/openai/CLIP/issues/83\n",
    "    # Mini-batching thanks to https://github.com/crowsonkb / https://twitter.com/RiversHaveWings\n",
    "    # Multi-GPU support: https://github.com/MicPie/clasp\n",
    "    def training_step(self, train_batch, idx):\n",
    "        # get optimizers and scheduler\n",
    "        optimizer = self.optimizers()\n",
    "\n",
    "        image, text = train_batch\n",
    "        n = math.ceil(len(image) // self.minibatch_size)\n",
    "        image_mbs = torch.chunk(image, n)\n",
    "        text_mbs = torch.chunk(text, n)\n",
    "\n",
    "        # calculate original statistics\n",
    "        with torch.no_grad():\n",
    "            ims = [F.normalize(self.model.encode_image(im), dim=1) for im in image_mbs]\n",
    "            txt = [F.normalize(self.model.encode_text(t), dim=1) for t in text_mbs]\n",
    "            # gather from all GPUs\n",
    "            ims = self.all_gather(torch.cat(ims))\n",
    "            txt = self.all_gather(torch.cat(txt))\n",
    "\n",
    "            if len(ims.shape) == 3:\n",
    "                ims = list(ims)\n",
    "                txt = list(txt)\n",
    "            else:\n",
    "                ims = [ims]\n",
    "                txt = [txt]\n",
    "\n",
    "            image_logits = torch.cat(ims) @ torch.cat(txt).t() * self.model.logit_scale.exp()\n",
    "            ground_truth = torch.arange(len(image_logits)).type_as(image_logits).long()\n",
    "            loss = (F.cross_entropy(image_logits, ground_truth) + F.cross_entropy(image_logits.t(), ground_truth)).div(2)\n",
    "            acc_i = (torch.argmax(image_logits, 1) == ground_truth).sum()\n",
    "            acc_t = (torch.argmax(image_logits, 0) == ground_truth).sum()\n",
    "            self.log_dict({'loss': loss / len(ims), 'acc': (acc_i + acc_t) / 2 / len(image) / len(ims)}, prog_bar=True)\n",
    "\n",
    "        if isinstance(optimizer, list):\n",
    "            optimizer = optimizer[0]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # image loss\n",
    "        for j, mb in enumerate(image_mbs):\n",
    "            images_tmp = copy.deepcopy(ims)\n",
    "            images_tmp[self.global_rank][j*self.minibatch_size:(j+1)*self.minibatch_size] = F.normalize(self.model.encode_image(mb), dim=1)\n",
    "            image_logits = torch.cat(images_tmp) @ torch.cat(txt).t() * self.model.logit_scale.exp()\n",
    "            ground_truth = torch.arange(len(image_logits)).type_as(image_logits).long()\n",
    "            loss = (F.cross_entropy(image_logits, ground_truth) + F.cross_entropy(image_logits.t(), ground_truth))/2\n",
    "            self.manual_backward(loss)\n",
    "\n",
    "        # text loss\n",
    "        for j, mb in enumerate(text_mbs):\n",
    "            text_tmp = copy.deepcopy(txt)\n",
    "            text_tmp[self.global_rank][j*self.minibatch_size:(j+1)*self.minibatch_size] = F.normalize(self.model.encode_text(mb), dim=1)\n",
    "            image_logits = torch.cat(ims) @ torch.cat(text_tmp).t() * self.model.logit_scale.exp()\n",
    "            loss = (F.cross_entropy(image_logits, ground_truth) + F.cross_entropy(image_logits.t(), ground_truth))/2\n",
    "            self.manual_backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler = self.lr_schedulers()\n",
    "        lr_scheduler.step()\n",
    "        self.model.logit_scale.data.clamp_(-np.log(100), np.log(100))\n",
    "\n",
    "    def validation_step(self, val_batch, idx):\n",
    "        image, text = val_batch\n",
    "        image_logits, text_logits = self.forward(image, text)\n",
    "        ground_truth = torch.arange(len(image_logits))\n",
    "        loss = (F.cross_entropy(image_logits, ground_truth) + F.cross_entropy(text_logits, ground_truth)).div(2)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = {\n",
    "            \"RN50\": 5e-4,\n",
    "            \"RN101\": 5e-4,\n",
    "            \"RN50x4\": 5e-4,\n",
    "            \"RN50x16\": 4e-4,\n",
    "            \"RN50x64\": 3.6e-4,\n",
    "            \"ViT-B/32\": 5e-4,\n",
    "            \"ViT-B/16\": 5e-4,\n",
    "            \"ViT-L/14\": 4e-4,\n",
    "            \"ViT-L/14-336px\": 2e-5\n",
    "        }[self.model_name]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=lr,\n",
    "            betas=(\n",
    "                0.9,\n",
    "                0.98 if self.isViT else 0.999\n",
    "            ),\n",
    "            eps=1e-6 if self.isViT else 1e-8,\n",
    "            weight_decay=0.2\n",
    "        )\n",
    "\n",
    "        # Source: https://github.com/openai/CLIP/issues/107\n",
    "        # Use pip install 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup'\n",
    "        lr_scheduler = CosineAnnealingWarmupRestarts(\n",
    "            optimizer,\n",
    "            first_cycle_steps=self.num_training_steps,\n",
    "            cycle_mult=1.0,\n",
    "            max_lr=lr,\n",
    "            min_lr=0,\n",
    "            warmup_steps=2000\n",
    "        )\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4fe9a2-d64b-4796-8c86-28419d5f600e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ff2fe-d76b-4823-9266-d452efbf22ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99f3aa-d327-4e9d-b33d-d8874340cd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ebe5c-345e-4c79-a25d-b3a5ec7f1ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef78f6-dd02-4016-bffc-6062632409ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8be17f6b-8367-4c22-8123-563ec021f649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88bfaf-e573-465a-95fa-115b058b1bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e3024965-045c-419d-97dc-f4ed448c9c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "12cb474a-6dc2-45e5-b009-afe5bfa14562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b1bb4ab-736a-4509-a13b-c4c0cb31240a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3b446890-dcf6-4d5d-b47e-57ff79c7b409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd681f-e0a0-44f2-8bff-f2717d5fb776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacf339-ad08-4aa9-a527-f0c6366140dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d173e-7d58-42a3-98d0-a496462cb3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46671ee0-8aec-474f-ba5e-8e6e17351741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
