{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f91838-b496-4a2e-b411-be32afa1b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from PIL import Image\n",
    "from urllib import request\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision, torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import time\n",
    "# proxy=\"http://sysproxy.wal-mart.com:8080\"\n",
    "# os.environ['http_proxy'] = proxy \n",
    "# os.environ['HTTP_PROXY'] = proxy\n",
    "# os.environ['https_proxy'] = proxy\n",
    "# os.environ['HTTPS_PROXY'] = proxy\n",
    "\n",
    "\n",
    "# export HTTP_PROXY=http://sysproxy.wal-mart.com:8080\n",
    "# export HTTPS_PROXY=http://sysproxy.wal-mart.com:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a96ce7f-76fc-4d72-9155-ab3380d9e597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gtin', 'Acceptable Temperature Range High',\n",
      "       'Acceptable Temperature Range UOM', 'Added Sugar Per Serving',\n",
      "       'Is Aerosol', 'Food Allergen Statements', 'alternate_shelves',\n",
      "       'Assembled Product Height', 'Assembled Product Length',\n",
      "       'Assembled Product Weight',\n",
      "       ...\n",
      "       'Maximum Motor Speed', 'Maximum Temperature', 'Receiver Compatibility',\n",
      "       'Reserve Capacity', 'SAE DOT-Compliant', 'Shackle Clearance',\n",
      "       'Shackle Diameter', 'Shackle Length', 'Shank Length', 'Shear Strength'],\n",
      "      dtype='object', length=1415)\n"
     ]
    }
   ],
   "source": [
    "from pprintpp import pprint\n",
    "gtin_mapping=pd.read_csv(os.path.join(os.getcwd(),\"dvc-manual/gtin_attr.csv\"))\n",
    "cols= gtin_mapping.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a599eb0d-4a97-47c7-97fe-83b93648cf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container Type\n",
      "Has Fuel Container\n",
      "Servings Per Container\n",
      "Container Material\n",
      "Rigid Plastic Packaging Container Indicator\n",
      "360 Image Container\n"
     ]
    }
   ],
   "source": [
    "cols= gtin_mapping.columns\n",
    "for col in cols:\n",
    "    if 'container' in col.lower():\n",
    "        print(col)\n",
    "    \n",
    "# [col for cols if 'shape' is in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba6d70b5-8257-40af-8bf5-b88c06b504df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1522\n"
     ]
    }
   ],
   "source": [
    "print(gtin_mapping.shape[0])\n",
    "col=\"Container Type\"\n",
    "categories=gtin_mapping.loc[gtin_mapping[col].notna(),[\"gtin\", \"Product Name\" ,\"Container Type\",\"Retail Packaging\",\"Shape\",\"Gemstone Shape\", \"Pearl Shape\"]][col].value_counts().reset_index()\n",
    "categories.to_csv(\"shape_categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e17c3ba-350c-453b-a447-73e4c1c9e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: DtypeWarning: Columns (0,27,49,73,144,166,182,193,231,243,270,276,327,328,330,345,390,393,400,402,404,406,423,426,427,438,445,446,452,459,460,466,469,483,488,489,490,491,493,494,498,500,501,502,503,505,507,509,510,511,513,519,528,534,537,546,575,580,586,591,594,605,614,619,621,627,629,630,639,640,641,642,660,661,663,666,667,672,676,684,685,687,689,694,697,699,700,701,705,706,707,710,712,713,715,717,719,724,726,730,732,733,736,737,740,741,743,744,745,746,748,750,752,753,755,756,760,761,762,763,765,767,770,771,772,778,780,781,782,783,785,788,793,794,796,797,798,799,800,804,805,806,807,808,809,811,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,832,833,834,835,836,837,838,839,840,841,842,844,845,846,848,849,850,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,868,869,870,871,872,874,877,881,882,885,889,891,892,907,908,911,916,917,923,924,931,933,934,949,951,957,958,965,967,968,970,973,974,978,984,986,987,994,1003,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1020,1024,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1047,1050,1051,1055,1057,1058,1059,1061,1064,1065,1067,1071,1074,1079,1080,1085,1092,1093,1109,1110,1113,1114,1123,1135,1136,1140,1143,1145,1147,1153,1154,1155,1156,1157,1159,1169,1170,1172,1175,1179,1180,1182,1184,1187,1190,1191,1223,1224,1227,1230,1233,1235,1236,1239,1240,1242,1247,1248,1260,1261,1269,1271,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1291,1292,1293,1294,1297,1298,1300,1301,1302,1303,1304,1305,1307,1308,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1324,1325,1326,1327,1328,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1354,1355,1356,1357,1358,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1406,1407,1408,1409,1410,1411,1412,1413,1414) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gtin's with no description: ['00016000124790' '00044700360019' '00703820010944']\n",
      "Shape before removing gtin's10661\n",
      "Shape after removing gtin's10132\n",
      "Shape after removing na from product name column 10132\n",
      "Gtin's with no description: ['00016000124790' '00044700360019' '00703820010944']\n",
      "Shape before removing gtin's34401\n",
      "Shape after removing gtin's32691\n",
      "Shape after removing na from product name column 32691\n",
      "Gtin's with no description: ['00016000124790' '00044700360019' '00703820010944']\n",
      "Shape before removing gtin's7914\n",
      "Shape after removing gtin's7520\n",
      "Shape after removing na from product name column 7520\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# import pandas as pd\n",
    "# gtin_mapping=pd.read_csv(os.path.join(os.getcwd(),\"dvc-manual/520_gtin_product_name.csv\"))\n",
    "# [gtin_mapping[gtin_mapping[\"gtin\"]==gtin_mapping[\"gtin\"].value_counts().index[1]][\"product_name\"].iloc[i] for i in range(0,4)]\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    CLEANR = re.compile('<.*?>') # remove html tags\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    pattern= r\"\\d*\\.\\d+\"# r'[0-9]' # remove decimal numbers\n",
    "    cleantext=re.sub(pattern, \"\",cleantext)\n",
    "    pattern= r'[0-9]' # remove any digits\n",
    "    cleantext=re.sub(pattern, \"\",cleantext)\n",
    "    return cleantext\n",
    "\n",
    "def get_mapping():\n",
    "    \n",
    "    gtin_mapping=pd.read_csv(os.path.join(os.getcwd(),\"dvc-manual/gtin_attr.csv\"))\n",
    "    desc_columns = [col for col in gtin_mapping.columns if 'desc' in col.lower() or \"name\" in col.lower() or \"date\" in col.lower()]\n",
    "    desc_columns= [\"gtin\",\"KARF Picker Description\",\"Product Long Description\",\"Short Description\",\"Product Name\" ]\n",
    "    gtin_mapping= gtin_mapping.loc[:,desc_columns]\n",
    "    \n",
    "    \n",
    "    gtin_mapping[\"Product Long Description\"]=gtin_mapping.apply(lambda x: cleanhtml(str(x[\"Product Long Description\"])), axis=1)\n",
    "    gtin_mapping[\"Short Description\"]=gtin_mapping.apply(lambda x: cleanhtml(str(x[\"Short Description\"])), axis=1)\n",
    "    gtin_mapping[\"Product Name\"]=gtin_mapping.apply(lambda x: cleanhtml(str(x[\"Product Name\"])), axis=1)\n",
    "    gtin_mapping=gtin_mapping.rename(columns={\"Product Long Description\": \"desc_long\", \"Short Description\": \"desc_short\", \"Product Name\": \"name\",\"KARF Picker Description\":\"desc_karf\" })\n",
    "\n",
    "    gtin_mapping[\"desc_long\"]=gtin_mapping.apply(lambda x: str(x[\"desc_long\"]).replace(\"|\", \"\"),axis=1)\n",
    "    gtin_mapping[\"desc_short\"]=gtin_mapping.apply(lambda x: str(x[\"desc_short\"]).replace(\"|\", \"\"),axis=1)\n",
    "    gtin_mapping[\"desc_karf\"]=gtin_mapping.apply(lambda x: str(x[\"desc_karf\"]).replace(\"|\", \"\"),axis=1)\n",
    "    gtin_mapping[\"name\"]=gtin_mapping.apply(lambda x: str(x[\"name\"]).replace(\"|\", \"\"),axis=1)\n",
    "\n",
    "    gtin_mapping[\"gtin\"]=gtin_mapping[\"gtin\"].apply(lambda x: str(x).zfill(14))\n",
    "    \n",
    "    return gtin_mapping\n",
    "\n",
    "def create_dataframe(path,label_col):\n",
    "    loader= torchvision.datasets.ImageFolder(root=path)\n",
    "    \n",
    "    df= pd.DataFrame(loader.imgs, columns= [\"img_path\", \"label\"])\n",
    "    df[\"gtin\"]= df.apply(lambda x: x[\"img_path\"].split(\"/\")[-2],axis=1)\n",
    "    df[\"gtin\"]=df[\"gtin\"].apply(lambda x: str(x).zfill(14))\n",
    "    \n",
    "    gtin_mapping= get_mapping()\n",
    "    \n",
    "    df= df.merge(gtin_mapping, left_on=['gtin'], right_on=[\"gtin\"], how = \"left\")\n",
    "    no_desc_gtin= df[df['name'].isna()].gtin.unique()\n",
    "    print(f\"Gtin's with no description: {no_desc_gtin}\")\n",
    "    print(f\"Shape before removing gtin's{df.shape[0]}\")\n",
    "    df= df[~df[\"gtin\"].isin(no_desc_gtin)]\n",
    "    print(f\"Shape after removing gtin's{df.shape[0]}\")\n",
    "    df = df[df[label_col].notna()]\n",
    "    print(f\"Shape after removing na from product name column {df.shape[0]}\")\n",
    "    key_list=[k for k in range(df.shape[0])]\n",
    "    df[\"key\"]= key_list\n",
    "    return df\n",
    "\n",
    "label_col= \"name\"\n",
    "test_path= os.path.join(os.getcwd(),'dvc-manual/gtin_60/data/prep/80gtin_allsubfolders/splitfolders/test')\n",
    "test_df=create_dataframe(test_path, label_col)\n",
    "\n",
    "train_path= os.path.join(os.getcwd(),'dvc-manual/gtin_60/data/prep/80gtin_allsubfolders/splitfolders/train')\n",
    "train_df=create_dataframe(train_path, label_col)\n",
    "\n",
    "val_path= os.path.join(os.getcwd(),'dvc-manual/gtin_60/data/prep/80gtin_allsubfolders/splitfolders/val')\n",
    "val_df=create_dataframe(val_path, label_col)\n",
    "\n",
    "## remove gtin's with no description available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8891468-3f6e-475a-9caa-121ea33b5eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nutella Chocolate Hazelnut Spread Perfect on Pancakes  oz Jar'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # batch_num=0\n",
    "# # for batch in train_dataloader:\n",
    "# #     batch_num+=1\n",
    "# #     if batch_num==52:\n",
    "# #         print(batch[1])\n",
    "# from transformers import CLIPTokenizer, CLIPTextModel\n",
    "# text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "# vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "from random import randint, choice\n",
    "descriptions = train_dataloader.dataset.caption[0].split('\\n')\n",
    "descriptions = list(filter(lambda t: len(t) > 0, descriptions))\n",
    "description = choice(descriptions)\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "66db2b47-dcd4-4438-a7c8-ec677b414550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>gtin</th>\n",
       "      <th>desc_karf</th>\n",
       "      <th>desc_long</th>\n",
       "      <th>desc_short</th>\n",
       "      <th>name</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jupyter/dvc-manual/gtin_60/data/prep/80g...</td>\n",
       "      <td>0</td>\n",
       "      <td>00009800800254</td>\n",
       "      <td>Nutella 33.5 oz</td>\n",
       "      <td>One  oz jar of delicious Nutella hazelnut spre...</td>\n",
       "      <td>The great taste of Nutella makes weekend break...</td>\n",
       "      <td>Nutella Chocolate Hazelnut Spread Perfect on P...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path  label            gtin  \\\n",
       "0  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "1  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "2  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "3  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "4  /home/jupyter/dvc-manual/gtin_60/data/prep/80g...      0  00009800800254   \n",
       "\n",
       "         desc_karf                                          desc_long  \\\n",
       "0  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "1  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "2  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "3  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "4  Nutella 33.5 oz  One  oz jar of delicious Nutella hazelnut spre...   \n",
       "\n",
       "                                          desc_short  \\\n",
       "0  The great taste of Nutella makes weekend break...   \n",
       "1  The great taste of Nutella makes weekend break...   \n",
       "2  The great taste of Nutella makes weekend break...   \n",
       "3  The great taste of Nutella makes weekend break...   \n",
       "4  The great taste of Nutella makes weekend break...   \n",
       "\n",
       "                                                name  key  \n",
       "0  Nutella Chocolate Hazelnut Spread Perfect on P...    0  \n",
       "1  Nutella Chocolate Hazelnut Spread Perfect on P...    1  \n",
       "2  Nutella Chocolate Hazelnut Spread Perfect on P...    2  \n",
       "3  Nutella Chocolate Hazelnut Spread Perfect on P...    3  \n",
       "4  Nutella Chocolate Hazelnut Spread Perfect on P...    4  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list=[k for k in range(train_df.shape[0])]\n",
    "train_df[\"key\"]= key_list\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1b10f-fece-4d64-a6e9-067012613676",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- CLIP encoder, CLIP text encoder, CLIP tokenizer, CLIP processor\n",
    "\n",
    "1. Prepare data into the required format\n",
    "2. Load text encoder and image encoder\n",
    "    Image processor CLIP preprocess\n",
    "    CLIP tokenizer\n",
    "        Default: clip.tokenize\n",
    "        Huggingface: \n",
    "        \n",
    "root_folder\n",
    "    img1.png\n",
    "    img1.txt\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10e6958b-8116-4a72-b011-5e1211b0d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "BATCH_SIZE=64\n",
    "\n",
    "# class image_caption_dataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "\n",
    "#         self.images = df[\"img_path\"].tolist()\n",
    "#         self.caption = df[\"name\"].tolist()\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.caption)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images = preprocess(Image.open(self.images[idx])) #preprocess from clip.load\n",
    "#         caption = self.caption[idx]\n",
    "#         return images,caption\n",
    "\n",
    "\n",
    "# class image_caption_dataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "\n",
    "#         self.images = df[\"img_path\"].tolist()\n",
    "#         self.caption = df[\"name\"].tolist()\n",
    "#         self.device = (\n",
    "#             \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         )  # If using GPU then use mixed precision training.\n",
    "#         _, self.preprocess = clip.load(\n",
    "#             \"ViT-B/32\", device=self.device, jit=False\n",
    "#         )  # Must set jit=False for training\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.caption)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images = self.preprocess(Image.open(self.images[idx]))  # preprocess from clip.load\n",
    "#         # caption=torch.cat([clip.tokenize(c) for c in self.caption])\n",
    "#         caption=clip.tokenize(self.caption[idx])[0]\n",
    "#         # caption = self.caption[idx]\n",
    "#         return images, caption\n",
    "\n",
    "class image_caption_dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.images = df[\"img_path\"].tolist()\n",
    "        self.caption = df[\"name\"].tolist()\n",
    "        self.keys= df[\"key\"].tolist()\n",
    "        self.image_transform = T.Compose([\n",
    "            T.Lambda(self.fix_img),\n",
    "            T.RandomResizedCrop(image_size,\n",
    "                            scale=(self.resize_ratio, 1.),\n",
    "                            ratio=(1., 1.)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "            ])\n",
    "        # self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.device = (\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )  # If using GPU then use mixed precision training.\n",
    "        _, self.preprocess = clip.load(\n",
    "            \"ViT-B/32\", device=self.device, jit=False\n",
    "        )  # Must set jit=False for training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        images = self.preprocess(Image.open(self.images[idx]))\n",
    "        caption=clip.tokenize(self.caption[idx])[0]\n",
    "        \n",
    "        # caption = self.caption[idx]\n",
    "        # caption= self.tokenizer(self.caption[idx])[0]\n",
    "        \n",
    "        return images, caption\n",
    "\n",
    "\n",
    "dataset = image_caption_dataset(train_df.loc[:,[\"img_path\",\"name\"]])\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n",
    "dataset = image_caption_dataset(val_df.loc[:,[\"img_path\",\"name\"]])\n",
    "val_loader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n",
    "dataset = image_caption_dataset(test_df.loc[:,[\"img_path\",\"name\"]])\n",
    "test_loader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cba52e6d-9836-44b9-b092-e30844f44c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(train_df[\"name\"].iloc[0:63])\n",
    "text=tokenizer([row[1] for row in train_df[\"name\"].iloc[0:64]], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "text_mbs_ids = torch.chunk(torch.arange(64), 1)\n",
    "\n",
    "text_mbs = []\n",
    "for s in text_mbs_ids:\n",
    "    d = {}\n",
    "    for key in list(text.keys()):\n",
    "        d[key] = text[key][s]\n",
    "    text_mbs.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00367cf9-64f3-4288-bfbc-e36a236203e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {k: v for k, v in zip(train_df.loc[:,[\"key\"]],train_df.loc[:,[\"img_path\"]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "424a2a36-2c14-4f41-8345-55ef348065df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_392/1046197491.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"johngiorgi/declutr-sci-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(tokenizer(row[1] for row in batch, padding=True, truncation=True, return_tensors=\"pt\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             raise ValueError(\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"johngiorgi/declutr-sci-base\")\n",
    "for batch in train_dataloader:\n",
    "    print(tokenizer(batch[1][1]))\n",
    "    #print(tokenizer(row[1] for row in batch, padding=True, truncation=True, return_tensors=\"pt\"))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42a907a3-40b2-407c-b352-882cb93daf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataloader.batch_size)\n",
    "# n = math.ceil(train_dataloader.batch_size// 64)\n",
    "# text_mbs_ids = torch.chunk(torch.arange(64), 32)\n",
    "\n",
    "# for s in text_mbs_ids:\n",
    "#     d = {}\n",
    "#     for key in list(text.keys()):\n",
    "#         d[key] = text[key][s]\n",
    "#     text_mbs.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb9b329c-eba0-43d5-bdc8-6320534e9215",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=np.unique(np.array(train_dataloader.dataset.caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fb3361c-18fa-408e-8d30-25379fad67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "\n",
    "# for train_batch in train_dataloader:\n",
    "#     image, text = train_batch\n",
    "#     print(f\"Length of image batch is {image.shape}\")\n",
    "#     print(f\"Length of text batch is {text.shape}\")\n",
    "#     n = math.ceil(len(image) // 128)\n",
    "#     print(n)\n",
    "#     image_mbs = torch.chunk(image, n)\n",
    "#     text_mbs = torch.chunk(text, n)\n",
    "\n",
    "#     # calculate original statistics\n",
    "#     # with torch.no_grad():\n",
    "#     #     ims = [F.normalize(self.model.encode_image(im), dim=1) for im in image_mbs]\n",
    "#     #     txt = [F.normalize(self.model.encode_text(t), dim=1) for t in text_mbs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a638887-2267-458d-84be-3919dc2bd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"ViT-B/32\",jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "# # from torchvision.models import resnet50\n",
    "# # img_encoder = resnet50(pretrained=True)\n",
    "# # img_encoder.fc = torch.nn.Linear(2048, 768)\n",
    "# # print(img_encoder.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8e2ce89-8d51-4bf6-8c5e-99405798c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 510/511 [10:28<00:01,  1.23s/batch, loss=4.16]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (51) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21645/2838219546.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (51) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "else :\n",
    "    clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "EPOCH=150\n",
    "\n",
    "\n",
    "print(\"Start Training\")\n",
    "for epoch in range(EPOCH):\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for list_image, list_txt in tepoch:\n",
    "        # for batch in train_dataloader:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            optimizer.zero_grad()\n",
    "            # batch_num+=1\n",
    "            # print(f'Batch number = {batch_num}')\n",
    "\n",
    "            # list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "\n",
    "            images=list_image.to(device)\n",
    "            # images= torch.stack([preprocess(Image.fromarray(img)) for img in list_image],dim=0).to(device) # omit the Image.fromarray if the images already in PIL format, change this line to images=list_image if using preprocess inside the dataset class\n",
    "            texts = clip.tokenize(list_txt).to(device)\n",
    "\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            ground_truth = torch.arange(len(list_image),dtype=torch.long,device=device)\n",
    "\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            acc_i = (torch.argmax(image_logits, 1) == ground_truth).sum()\n",
    "            acc_t = (torch.argmax(image_logits, 0) == ground_truth).sum()\n",
    "            total_loss.backward()\n",
    "            if device == \"cpu\":\n",
    "                optimizer.step()\n",
    "            else : \n",
    "                convert_models_to_fp32(model)\n",
    "                optimizer.step()\n",
    "                clip.model.convert_weights(model)\n",
    "            tepoch.set_postfix(loss=total_loss.item())\n",
    "            time.sleep(0.1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "01d7c529-b0ed-4759-ba7e-0cc0241e6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acac20b1-20e5-4ef4-8465-c67c4fda4ffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_392/1507401090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a6942-6808-4ba2-a9d8-b9d156a2e69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fa510-fb76-4c7b-aeb4-62a5c1a36ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPWrapper(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 config: dict,\n",
    "                 minibatch_size: int\n",
    "                 ):\n",
    "        \"\"\"A lightning wrapper for a CLIP model as specified in the paper.\n",
    "        Args:\n",
    "            model_name (str): A case sensitive visual model name.\n",
    "            config (dict): A dictionary containing the CLIP instantiation parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = CLIP(**config)\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.isViT = 'ViT' in self.model_name\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "    \n",
    "    # Sourced from https://github.com/PyTorchLightning/pytorch-lightning/issues/5449\n",
    "    @property\n",
    "    def num_training_steps(self) -> int:\n",
    "        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n",
    "        dataset = self.train_dataloader()\n",
    "        if self.trainer.max_steps:\n",
    "            return self.trainer.max_steps\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "\n",
    "        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)\n",
    "        if self.trainer.tpu_cores:\n",
    "            num_devices = max(num_devices, self.trainer.tpu_cores)\n",
    "\n",
    "        effective_batch_size = dataset.batch_size * self.trainer.accumulate_grad_batches * num_devices\n",
    "        return (dataset_size // effective_batch_size) * self.trainer.max_epochs\n",
    "\n",
    "    # Training loss: https://github.com/openai/CLIP/issues/83\n",
    "    # Mini-batching thanks to https://github.com/crowsonkb / https://twitter.com/RiversHaveWings\n",
    "    # Multi-GPU support: https://github.com/MicPie/clasp\n",
    "    def training_step(self, train_batch, idx):\n",
    "        # get optimizers and scheduler\n",
    "        optimizer = self.optimizers()\n",
    "\n",
    "        image, text = train_batch\n",
    "        n = math.ceil(len(image) // self.minibatch_size)\n",
    "        image_mbs = torch.chunk(image, n)\n",
    "        text_mbs = torch.chunk(text, n)\n",
    "\n",
    "        # calculate original statistics\n",
    "        with torch.no_grad():\n",
    "            ims = [F.normalize(self.model.encode_image(im), dim=1) for im in image_mbs]\n",
    "            txt = [F.normalize(self.model.encode_text(t), dim=1) for t in text_mbs]\n",
    "            # gather from all GPUs\n",
    "            ims = self.all_gather(torch.cat(ims))\n",
    "            txt = self.all_gather(torch.cat(txt))\n",
    "\n",
    "            if len(ims.shape) == 3:\n",
    "                ims = list(ims)\n",
    "                txt = list(txt)\n",
    "            else:\n",
    "                ims = [ims]\n",
    "                txt = [txt]\n",
    "\n",
    "            image_logits = torch.cat(ims) @ torch.cat(txt).t() * self.model.logit_scale.exp()\n",
    "            ground_truth = torch.arange(len(image_logits)).type_as(image_logits).long()\n",
    "            loss = (F.cross_entropy(image_logits, ground_truth) + F.cross_entropy(image_logits.t(), ground_truth)).div(2)\n",
    "            acc_i = (torch.argmax(image_logits, 1) == ground_truth).sum()\n",
    "            acc_t = (torch.argmax(image_logits, 0) == ground_truth).sum()\n",
    "            self.log_dict({'loss': loss / len(ims), 'acc': (acc_i + acc_t) / 2 / len(image) / len(ims)}, prog_bar=True)\n",
    "\n",
    "        if isinstance(optimizer, list):\n",
    "            optimizer = optimizer[0]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # image loss\n",
    "        for j, mb in enumerate(image_mbs):\n",
    "            images_tmp = copy.deepcopy(ims)\n",
    "            images_tmp[self.global_rank][j*self.minibatch_size:(j+1)*self.minibatch_size] = F.normalize(self.model.encode_image(mb), dim=1)\n",
    "            image_logits = torch.cat(images_tmp) @ torch.cat(txt).t() * self.model.logit_scale.exp()\n",
    "            ground_truth = torch.arange(len(image_logits)).type_as(image_logits).long()\n",
    "            loss = (F.cross_entropy(image_logits, ground_truth) + F.cross_entropy(image_logits.t(), ground_truth))/2\n",
    "            self.manual_backward(loss)\n",
    "\n",
    "        # text loss\n",
    "        for j, mb in enumerate(text_mbs):\n",
    "            text_tmp = copy.deepcopy(txt)\n",
    "            text_tmp[self.global_rank][j*self.minibatch_size:(j+1)*self.minibatch_size] = F.normalize(self.model.encode_text(mb), dim=1)\n",
    "            image_logits = torch.cat(ims) @ torch.cat(text_tmp).t() * self.model.logit_scale.exp()\n",
    "            loss = (F.cross_entropy(image_logits, ground_truth) + F.cross_entropy(image_logits.t(), ground_truth))/2\n",
    "            self.manual_backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler = self.lr_schedulers()\n",
    "        lr_scheduler.step()\n",
    "        self.model.logit_scale.data.clamp_(-np.log(100), np.log(100))\n",
    "\n",
    "    def validation_step(self, val_batch, idx):\n",
    "        image, text = val_batch\n",
    "        image_logits, text_logits = self.forward(image, text)\n",
    "        ground_truth = torch.arange(len(image_logits))\n",
    "        loss = (F.cross_entropy(image_logits, ground_truth) + F.cross_entropy(text_logits, ground_truth)).div(2)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = {\n",
    "            \"RN50\": 5e-4,\n",
    "            \"RN101\": 5e-4,\n",
    "            \"RN50x4\": 5e-4,\n",
    "            \"RN50x16\": 4e-4,\n",
    "            \"RN50x64\": 3.6e-4,\n",
    "            \"ViT-B/32\": 5e-4,\n",
    "            \"ViT-B/16\": 5e-4,\n",
    "            \"ViT-L/14\": 4e-4,\n",
    "            \"ViT-L/14-336px\": 2e-5\n",
    "        }[self.model_name]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=lr,\n",
    "            betas=(\n",
    "                0.9,\n",
    "                0.98 if self.isViT else 0.999\n",
    "            ),\n",
    "            eps=1e-6 if self.isViT else 1e-8,\n",
    "            weight_decay=0.2\n",
    "        )\n",
    "\n",
    "        # Source: https://github.com/openai/CLIP/issues/107\n",
    "        # Use pip install 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup'\n",
    "        lr_scheduler = CosineAnnealingWarmupRestarts(\n",
    "            optimizer,\n",
    "            first_cycle_steps=self.num_training_steps,\n",
    "            cycle_mult=1.0,\n",
    "            max_lr=lr,\n",
    "            min_lr=0,\n",
    "            warmup_steps=2000\n",
    "        )\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4fe9a2-d64b-4796-8c86-28419d5f600e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ff2fe-d76b-4823-9266-d452efbf22ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acd5d6be-96cb-4bdc-ab91-73aeb365724f",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "139c681c-021b-4691-9af5-9a110a28e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPTokenizer, CLIPProcessor\n",
    "# import pandas as pd\n",
    "# gtin_mapping=pd.read_csv(os.path.join(os.getcwd(),\"dvc-manual/520_gtin_product_name.csv\"))\n",
    "# [gtin_mapping[gtin_mapping[\"gtin\"]==gtin_mapping[\"gtin\"].value_counts().index[1]][\"product_name\"].iloc[i] for i in range(0,4)]\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    CLEANR = re.compile(\"<.*?>\")  # remove html tags\n",
    "    cleantext = re.sub(CLEANR, \"\", raw_html)\n",
    "    pattern = r\"\\d*\\.\\d+\"  # r'[0-9]' # remove decimal numbers\n",
    "    cleantext = re.sub(pattern, \"\", cleantext)\n",
    "    pattern = r\"[0-9]\"  # remove any digits\n",
    "    cleantext = re.sub(pattern, \"\", cleantext)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def get_mapping():\n",
    "    gtin_mapping = pd.read_csv(\"/home/jupyter/dvc-manual/gtin_attr.csv\")\n",
    "    desc_columns = [\n",
    "        col\n",
    "        for col in gtin_mapping.columns\n",
    "        if \"desc\" in col.lower() or \"name\" in col.lower() or \"date\" in col.lower()\n",
    "    ]\n",
    "    desc_columns = [\n",
    "        \"gtin\",\n",
    "        \"KARF Picker Description\",\n",
    "        \"Product Long Description\",\n",
    "        \"Short Description\",\n",
    "        \"Product Name\",\n",
    "    ]\n",
    "    gtin_mapping = gtin_mapping.loc[:, desc_columns]\n",
    "    gtin_mapping[\"Product Long Description\"] = gtin_mapping.apply(\n",
    "        lambda x: cleanhtml(str(x[\"Product Long Description\"])), axis=1\n",
    "    )\n",
    "    gtin_mapping[\"Short Description\"] = gtin_mapping.apply(\n",
    "        lambda x: cleanhtml(str(x[\"Short Description\"])), axis=1\n",
    "    )\n",
    "    gtin_mapping[\"Product Name\"] = gtin_mapping.apply(\n",
    "        lambda x: cleanhtml(str(x[\"Product Name\"])), axis=1\n",
    "    )\n",
    "    gtin_mapping = gtin_mapping.rename(\n",
    "        columns={\n",
    "            \"Product Long Description\": \"desc_long\",\n",
    "            \"Short Description\": \"desc_short\",\n",
    "            \"Product Name\": \"name\",\n",
    "            \"KARF Picker Description\": \"desc_karf\",\n",
    "        }\n",
    "    )\n",
    "    gtin_mapping[\"desc_long\"] = gtin_mapping.apply(\n",
    "        lambda x: str(x[\"desc_long\"]).replace(\"|\", \"\"), axis=1\n",
    "    )\n",
    "    gtin_mapping[\"desc_short\"] = gtin_mapping.apply(\n",
    "        lambda x: str(x[\"desc_short\"]).replace(\"|\", \"\"), axis=1\n",
    "    )\n",
    "    gtin_mapping[\"desc_karf\"] = gtin_mapping.apply(\n",
    "        lambda x: str(x[\"desc_karf\"]).replace(\"|\", \"\"), axis=1\n",
    "    )\n",
    "    gtin_mapping[\"name\"] = gtin_mapping.apply(\n",
    "        lambda x: str(x[\"name\"]).replace(\"|\", \"\"), axis=1\n",
    "    )\n",
    "    gtin_mapping[\"gtin\"] = gtin_mapping[\"gtin\"].apply(lambda x: str(x).zfill(14))\n",
    "    return gtin_mapping\n",
    "\n",
    "\n",
    "def create_dataframe(path, label_col):\n",
    "    loader = torchvision.datasets.ImageFolder(root=path)\n",
    "\n",
    "    df = pd.DataFrame(loader.imgs, columns=[\"img_path\", \"label\"])\n",
    "    df[\"gtin\"] = df.apply(lambda x: x[\"img_path\"].split(\"/\")[-2], axis=1)\n",
    "    df[\"gtin\"] = df[\"gtin\"].apply(lambda x: str(x).zfill(14))\n",
    "\n",
    "    gtin_mapping = get_mapping()\n",
    "\n",
    "    df = df.merge(gtin_mapping, left_on=[\"gtin\"], right_on=[\"gtin\"], how=\"left\")\n",
    "    no_desc_gtin = df[df[\"name\"].isna()].gtin.unique()\n",
    "    print(f\"Gtin's with no description: {no_desc_gtin}\")\n",
    "    print(f\"Shape before removing gtin's{df.shape[0]}\")\n",
    "    df = df[~df[\"gtin\"].isin(no_desc_gtin)]\n",
    "    print(f\"Shape after removing gtin's{df.shape[0]}\")\n",
    "    df = df[df[label_col].notna()]\n",
    "    print(f\"Shape after removing na from product name column {df.shape[0]}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4b2abd42-a7cd-422b-96a6-2a8a62a26d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:80: DtypeWarning: Columns (0,27,49,73,144,166,182,193,231,243,270,276,327,328,330,345,390,393,400,402,404,406,423,426,427,438,445,446,452,459,460,466,469,483,488,489,490,491,493,494,498,500,501,502,503,505,507,509,510,511,513,519,528,534,537,546,575,580,586,591,594,605,614,619,621,627,629,630,639,640,641,642,660,661,663,666,667,672,676,684,685,687,689,694,697,699,700,701,705,706,707,710,712,713,715,717,719,724,726,730,732,733,736,737,740,741,743,744,745,746,748,750,752,753,755,756,760,761,762,763,765,767,770,771,772,778,780,781,782,783,785,788,793,794,796,797,798,799,800,804,805,806,807,808,809,811,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,832,833,834,835,836,837,838,839,840,841,842,844,845,846,848,849,850,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,868,869,870,871,872,874,877,881,882,885,889,891,892,907,908,911,916,917,923,924,931,933,934,949,951,957,958,965,967,968,970,973,974,978,984,986,987,994,1003,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1020,1024,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1047,1050,1051,1055,1057,1058,1059,1061,1064,1065,1067,1071,1074,1079,1080,1085,1092,1093,1109,1110,1113,1114,1123,1135,1136,1140,1143,1145,1147,1153,1154,1155,1156,1157,1159,1169,1170,1172,1175,1179,1180,1182,1184,1187,1190,1191,1223,1224,1227,1230,1233,1235,1236,1239,1240,1242,1247,1248,1260,1261,1269,1271,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1291,1292,1293,1294,1297,1298,1300,1301,1302,1303,1304,1305,1307,1308,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1324,1325,1326,1327,1328,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1354,1355,1356,1357,1358,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1406,1407,1408,1409,1410,1411,1412,1413,1414) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gtin's with no description: ['00016000124790' '00044700360019' '00703820010944']\n",
      "Shape before removing gtin's34401\n",
      "Shape after removing gtin's32691\n",
      "Shape after removing na from product name column 32691\n"
     ]
    }
   ],
   "source": [
    "testing=create_dataframe(\"/home/jupyter/dvc-manual/gtin_60/data/prep/80gtin_allsubfolders/splitfolders/train\",\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ebe5c-345e-4c79-a25d-b3a5ec7f1ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 folder: str,\n",
    "                 image_size=224,\n",
    "                 resize_ratio=0.75,\n",
    "                 shuffle=False,\n",
    "                 custom_tokenizer=False\n",
    "                 ):\n",
    "        \"\"\"Create a text image dataset from a directory with congruent text and image names.\n",
    "\n",
    "        Args:\n",
    "            folder (str): Folder containing images and text files matched by their paths' respective \"stem\"\n",
    "            image_size (int, optional): The size of outputted images. Defaults to 224.\n",
    "            resize_ratio (float, optional): Minimum percentage of image contained by resize. Defaults to 0.75.\n",
    "            shuffle (bool, optional): Whether or not to have shuffling behavior during sampling. Defaults to False.\n",
    "            custom_tokenizer (bool, optional): Whether or not there is a custom tokenizer. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        train_df= create_dataframe(self.folder, \"name\")\n",
    "        self.keys = self.keys = list(train_df[\"key\"])\n",
    "        self.text_files = {k:v for k, v in zip(train_df[\"key\"],train_df[\"name\"])}\n",
    "        self.image_files = {k:v for k, v in zip(train_df[\"key\"],train_df[\"img_path\"])}\n",
    "        \n",
    "        self.resize_ratio = resize_ratio\n",
    "        self.image_transform = T.Compose([\n",
    "            T.Lambda(self.fix_img),\n",
    "            T.RandomResizedCrop(image_size,\n",
    "                                scale=(self.resize_ratio, 1.),\n",
    "                                ratio=(1., 1.)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "        self.custom_tokenizer = custom_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def fix_img(self, img):\n",
    "        return img.convert('RGB') if img.mode != 'RGB' else img\n",
    "\n",
    "    def random_sample(self):\n",
    "        return self.__getitem__(randint(0, self.__len__() - 1))\n",
    "\n",
    "    def sequential_sample(self, ind):\n",
    "        if ind >= self.__len__() - 1:\n",
    "            return self.__getitem__(0)\n",
    "        return self.__getitem__(ind + 1)\n",
    "\n",
    "    def skip_sample(self, ind):\n",
    "        if self.shuffle:\n",
    "            return self.random_sample()\n",
    "        return self.sequential_sample(ind=ind)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        key = self.keys[ind]\n",
    "\n",
    "        text_file = self.text_files[key]\n",
    "        image_file = self.image_files[key]\n",
    "\n",
    "        descriptions = text_file.read_text().split('\\n')\n",
    "        descriptions = list(filter(lambda t: len(t) > 0, descriptions))\n",
    "        try:\n",
    "            description = choice(descriptions)\n",
    "        except IndexError as zero_captions_in_file_ex:\n",
    "            print(f\"An exception occurred trying to load file {text_file}.\")\n",
    "            print(f\"Skipping index {ind}\")\n",
    "            return self.skip_sample(ind)\n",
    "\n",
    "        tokenized_text = description if self.custom_tokenizer else clip.tokenize(description)[0]\n",
    "\n",
    "        try:\n",
    "            image_tensor = self.image_transform(PIL.Image.open(image_file))\n",
    "        except (PIL.UnidentifiedImageError, OSError) as corrupt_image_exceptions:\n",
    "            print(f\"An exception occurred trying to load file {image_file}.\")\n",
    "            print(f\"Skipping index {ind}\")\n",
    "            return self.skip_sample(ind)\n",
    "\n",
    "        # Success\n",
    "        return image_tensor, tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b9ef78f6-dd02-4016-bffc-6062632409ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8be17f6b-8367-4c22-8123-563ec021f649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88bfaf-e573-465a-95fa-115b058b1bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e3024965-045c-419d-97dc-f4ed448c9c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "12cb474a-6dc2-45e5-b009-afe5bfa14562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b1bb4ab-736a-4509-a13b-c4c0cb31240a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3b446890-dcf6-4d5d-b47e-57ff79c7b409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd681f-e0a0-44f2-8bff-f2717d5fb776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacf339-ad08-4aa9-a527-f0c6366140dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d173e-7d58-42a3-98d0-a496462cb3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46671ee0-8aec-474f-ba5e-8e6e17351741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
